print("hi")

import argparse
import os
import sys
import time
from pathlib import Path
from typing import Optional

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
from torch.autograd.gradcheck import _test_undefined_backward_mode
from torch_geometric.nn import GINConv, MessagePassing

from build_logicGNN import *
from explain_gnn import *
from gnn import GAT, GCN, GIN, GraphSAGE  # your GCN class
from grounding import *
from load_data import load_data  # your function from before
from utils import load_model, test, train  # your train/test/save functions

# dataset-specific early stop thresholds
stop_dict = {
    "BAMultiShapes": 0.8,
    "BBBP": 0.96,
    "Mutagenicity": 0.76,
    "IMDB-BINARY": 0.7350,
    "NCI1": 0.7,
    "reddit_threads": 0.8,
    "twitch_egos": 0.8,
    "github_stargazers": 0.8,
    "SingleP4": 0.9,
}
original_atom_dict = {
    1: "H",
    5: "B",
    6: "C",
    7: "N",
    8: "O",
    9: "F",
    11: "Na",
    15: "P",
    16: "S",
    17: "Cl",
    20: "Ca",
    35: "Br",
    53: "I",
}
atom_types = sorted(original_atom_dict.keys())
atom_to_idx = {atom_num: idx for idx, atom_num in enumerate(atom_types)}
num_atom_types = len(atom_types)
BBBP_atom_type_dict = {
    idx: original_atom_dict[atom_types[idx]] for idx in range(num_atom_types)
}


def try_to_load_model_from_checkpoint_path(
    checkpoint_path: str, device: torch.device
) -> torch.nn.Module:
    from graph_learning.utils import load_checkpoint as load_model_checkpoint
    from graph_learning.utils.packaging import load_model_from_bundle
    
    # try multiple loading methods
    attempts = {
        "torch package bundle": lambda: load_model_from_bundle(
            checkpoint_path, map_location=str(device)
        ),
        "Bare Torch Load": lambda: torch.load(
            checkpoint_path, map_location=device, weights_only=False
        ),
        "Cloudpickle Custom Loader": lambda: load_model_checkpoint(
            checkpoint_path, device=str(device)
        ),
    }
    
    # Try to import ArchTestingModule if available - for .ckpt files
    try:
        from scripts.arch_study import ArchTestingModule
        attempts = {
            "ArchTestingModule": lambda: ArchTestingModule.load_from_checkpoint(
                checkpoint_path=checkpoint_path, map_location=device
            ).model,
            **attempts
        }
    except (ImportError, AttributeError):
        pass  # Skip this loading method if not available
    
    # Also try PyTorch Lightning checkpoint loading if available
    try:
        import pytorch_lightning as pl
        # Try loading as a Lightning checkpoint and extracting the model
        def load_from_lightning_ckpt():
            ckpt = torch.load(checkpoint_path, map_location=device)
            if 'state_dict' in ckpt:
                # This is a Lightning checkpoint - need to reconstruct the model
                # For now, skip this as we'd need model architecture info
                raise ValueError("Lightning checkpoint detected but model reconstruction not implemented")
            raise ValueError("Not a Lightning checkpoint")
        
        attempts = {"Lightning Checkpoint": load_from_lightning_ckpt, **attempts}
    except (ImportError, ValueError):
        pass

    last_exc = None
    for attempt_name, attempt in attempts.items():
        try:
            print(f"Trying to load model via: {attempt_name}")
            result = attempt()
            # Validate that we got a real model
            if not isinstance(result, torch.nn.Module):
                raise ValueError(f"Got {type(result)} instead of torch.nn.Module")
            print(f"Successfully loaded model via: {attempt_name}")
            break
        except Exception as e:
            print(f"  Error: {e}", file=sys.stderr)
            last_exc = e
    else:
        raise last_exc

    model: torch.nn.Module = result
    return model


def get_model_from_checkpoint(
    checkpoint_path: str, device: torch.device
) -> Optional[torch.nn.Module]:
    """Load model from a specific checkpoint path"""
    try:
        return try_to_load_model_from_checkpoint_path(checkpoint_path, device)
    except Exception as e:
        print(f"Failed to load model from {checkpoint_path}: {e}")
        return None


def fix_model(model: torch.nn.Module) -> torch.nn.Module:
    # normalize model architecture ... add needed fields
    # basically a stupid monkey path
    for module in model.modules():
        if isinstance(module, MessagePassing):
            if isinstance(module, GINConv):
                module.in_channels = module.nn[0].in_features
                module.out_channels = module.nn[-1].out_features
    return model


def main():
    parser = argparse.ArgumentParser(
        description="Train and evaluate GNNs on graph datasets"
    )

    # dataset and experiment setup
    parser.add_argument(
        "--dataset",
        type=str,
        default="SingleP4",
        choices=[
            "BBBP",
            "Mutagenicity",
            "IMDB-BINARY",
            "NCI1",
            "BAMultiShapes",
            "reddit_threads",
            "twitch_egos",
            "github_stargazers",
            "SingleP4",
        ],
        required=True,
    )
    parser.add_argument("--seed", type=int, default=0, help="Random seed")
    parser.add_argument(
        "--load", action="store_true", help="Load pretrained model instead of training"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint file to load pretrained model",
    )
    parser.add_argument(
        "--max_depth", type=int, default=5, help="Maximum depth for decision tree"
    )
    parser.add_argument(
        "--arch",
        type=str,
        choices=["GCN", "GIN", "GAT", "GraphSAGE"],
        default="GCN",
        help="GNN architecture to use",
    )
    parser.add_argument(
        "--plot", type=int, default=0, help="Whether to plot predicates"
    )

    args = parser.parse_args()

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.manual_seed(args.seed)

    # Load data
    train_dataset, test_dataset, train_loader, test_loader, device = load_data(
        args.dataset, args.seed
    )
    y_labels_flat = []
    for data in train_dataset:
        y_labels_flat.append(data.y.item())
    for data in test_dataset:
        y_labels_flat.append(data.y.item())

    class_weights = compute_class_weight(
        class_weight="balanced", classes=np.unique(y_labels_flat), y=y_labels_flat
    )
    class_weights_map = dict(zip(np.unique(y_labels_flat), class_weights))

    print(f"Correctly calculated class weights: {class_weights_map}")
    # ✅ Decide whether to use conv3 based on dataset
    use_conv3 = args.dataset in [
        "BAMultiShapes",
        "SingleP4",
    ]  # Use 3 layers for structural tasks

    # Load model from checkpoint if specified
    if args.checkpoint:
        print(f"Loading pretrained model from checkpoint: {args.checkpoint}")
        loaded = get_model_from_checkpoint(args.checkpoint, device)
        if loaded is None:
            raise ValueError(f"Failed to load model from checkpoint: {args.checkpoint}")

        # It's a full model
        print("Loaded full model object")
        model = loaded
        model = fix_model(model)
        model = model.to(device)

        test_acc = test(model, test_loader, device)
        print(f"Loaded pretrained model | Test Accuracy: {test_acc:.4f}")

    if args.dataset == "BBBP":
        atom_type_dict = BBBP_atom_type_dict
        one_hot = 1
        use_embed = 1
        k_hops = 2
    elif args.dataset == "BAMultiShapes":
        atom_type_dict = {}
        one_hot = 0
        use_embed = 1
        k_hops = 3
    elif args.dataset == "Mutagenicity":
        atom_type_dict = {
            0: "C",
            1: "O",
            2: "Cl",
            3: "H",
            4: "N",
            5: "F",
            6: "Br",
            7: "S",
            8: "P",
            9: "I",
            10: "Na",
            11: "K",
            12: "Li",
            13: "Ca",
        }
        one_hot = 1
        use_embed = 1
        k_hops = 2
    elif args.dataset == "IMDB-BINARY":
        atom_type_dict = {}
        one_hot = 0
        use_embed = 1
        k_hops = 2
    elif args.dataset == "NCI1":
        atom_type_dict = {i: i for i in range(37)}
        one_hot = 1
        use_embed = 1
        k_hops = 2
    elif args.dataset == "reddit_threads":
        atom_type_dict = {}
        one_hot = 0
        use_embed = 0
        k_hops = 2
    elif args.dataset == "twitch_egos":
        atom_type_dict = {}
        one_hot = 0
        use_embed = 0
        k_hops = 2
    elif args.dataset == "github_stargazers":
        atom_type_dict = {}
        one_hot = 0
        use_embed = 0
        k_hops = 2
    elif args.dataset == "SingleP4":
        # 10 (degree one-hot) + 1 (degree norm) + 1 (neighbor deg sum) + 1 (neighbor deg max) = 13 features
        atom_type_dict = {
            i: f"deg_{i}" for i in range(10)
        }  # Map one-hot positions to degree labels
        one_hot = 0  # Features already processed
        use_embed = 1  # Enable embedding patterns to distinguish P4 nodes
        k_hops = 3  # P4 has 3 edges, so k=3 captures the full pattern

    start_time = time.time()
    
    # Try to extract activations - skip if model doesn't support it
    try:
        (
            gnn_train_pred_tensor,
            train_y_tensor,
            train_x_dict,
            train_edge_dict,
            train_activations_dict,
            train_gnn_graph_embed,
        ) = get_all_activations_graph(train_loader, model, device)
        (
            gnn_test_pred_tensor,
            test_y_tensor,
            test_x_dict,
            test_edge_dict,
            test_activations_dict,
            test_gnn_graph_embed,
        ) = get_all_activations_graph(test_loader, model, device)
        activations_available = True
    except ValueError as e:
        if "does not return activations" in str(e):
            print(f"\n⚠️  Warning: {e}")
            print("Skipping explanation phase. Model evaluation completed successfully.")
            if args.checkpoint:
                print(f"\nFinal Test Accuracy: {test_acc:.4f}")
                return  # Exit early for pretrained models without activation support
            activations_available = False
        else:
            raise
    
    if not activations_available:
        print("Cannot proceed without activations. Exiting.")
        return
    
    save_dir_root = f"./plot/{args.dataset}/{args.seed}/{args.arch}"
    print(
        f"If the plot flag is set, explanation results will be saved to {save_dir_root}"
    )

    torch.save(
        {
            "pred_tensor": gnn_train_pred_tensor.cpu(),
            "y_tensor": train_y_tensor.cpu(),
            "x_dict": train_x_dict,
            "edge_dict": train_edge_dict,
            "activations_dict": train_activations_dict,
            "graph_embed": train_gnn_graph_embed,
        },
        os.path.join(save_dir_root, "train_results.pt"),
    )

    # Save testing results
    torch.save(
        {
            "pred_tensor": gnn_test_pred_tensor.cpu(),
            "y_tensor": test_y_tensor.cpu(),
            "x_dict": test_x_dict,
            "edge_dict": test_edge_dict,
            "activations_dict": test_activations_dict,
            "graph_embed": test_gnn_graph_embed,
        },
        os.path.join(save_dir_root, "test_results.pt"),
    )

    X = train_gnn_graph_embed

    # y = train_y_tensor
    y = gnn_train_pred_tensor  # explain gnn so use pred_tensor thats predicted results on train data by GNN

    # Train a Decision Tree Classifier
    clf = DecisionTreeClassifier(
        max_depth=1, random_state=42
    )  # Adjust max_depth as needed
    clf.fit(X, y)

    # Predict on the test set
    y_pred = clf.predict(X)
    # Compute accuracy
    ## make sure y and y_pred are numpy arrays use if if they are tensors
    if torch.is_tensor(y):
        y = y.cpu().numpy()
    if torch.is_tensor(y_pred):
        y_pred = y_pred.cpu().numpy()
    accuracy = accuracy_score(y, y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    clf, index_0_correct, index_1_correct = decision_tree_explainer(
        train_gnn_graph_embed,
        gnn_train_pred_tensor,
        test_gnn_graph_embed,
        gnn_test_pred_tensor,
        max_depth=1,
    )
    tree = clf.tree_
    val_idx = tree.feature[0]
    threshold = tree.threshold[0]
    y_pred = clf.predict(train_gnn_graph_embed)
    print(
        "Accuracy of decision tree on training set: ",
        accuracy_score(gnn_train_pred_tensor.cpu().numpy(), y_pred),
    )
    (
        predicates,
        predicate_to_idx,
        predicate_node,
        predicate_graph_class_0,
        predicate_graph_class_1,
        rules_matrix_0,
        rules_matrix_1,
    ) = get_predicates_bin_one_pass(
        index_0_correct,
        index_1_correct,
        train_x_dict,
        train_edge_dict,
        train_activations_dict,
        val_idx,
        threshold,
        use_embed=use_embed,
        k_hops=k_hops,
    )
    # Create mapping from predicate to index
    predicates_idx_mapping = {
        predicate: idx for idx, predicate in enumerate(predicates)
    }

    # Create mapping from index to predicate
    idx_predicates_mapping = {
        idx: predicate for idx, predicate in enumerate(predicates)
    }
    index_test = torch.tensor(list(range(test_y_tensor.shape[0])))
    test_predicate_graph, test_res = get_predicate_graph(
        index_test,
        predicates,
        predicate_to_idx,
        test_x_dict,
        test_edge_dict,
        test_activations_dict,
        val_idx,
        threshold,
        use_embed=use_embed,
        k_hops=k_hops,
    )
    leaf_rules_samples_0, leaf_rules_samples_1, used_predicates, clf_graph = (
        get_discriminative_rules_with_samples(
            rules_matrix_0,
            rules_matrix_1,
            index_0_correct,
            index_1_correct,
            max_depth=args.max_depth,
            plot=0,
            text=0,
        )
    )
    print("Discriminative rules learned from decision tree:")
    print("Class 0 rules:")
    print(leaf_rules_samples_0.keys())
    print("Class 1 rules:")
    print(leaf_rules_samples_1.keys())

    y_true_fid = gnn_test_pred_tensor.cpu().numpy()
    y_pred_fid = clf_graph.predict(test_res.T)

    fidelity_acc = accuracy_score(y_true_fid, y_pred_fid)
    prec_fid, rec_fid, f1_fid, _ = precision_recall_fscore_support(
        y_true_fid, y_pred_fid, average="weighted"
    )

    # -----------------------
    # Accuracy (vs ground truth)
    # -----------------------
    y_true = test_y_tensor.cpu().numpy()
    y_pred = clf_graph.predict(test_res.T)
    gnn_pred = gnn_test_pred_tensor.numpy().ravel()
    test_acc = accuracy_score(y_true, gnn_pred)

    weighted_accuracy = accuracy_score(
        y_true, y_pred, sample_weight=[class_weights_map[label] for label in y_true]
    )
    test_fid = accuracy_score(gnn_pred, y_pred)
    weighted_fidelity = accuracy_score(
        gnn_pred, y_pred, sample_weight=[class_weights_map[label] for label in y_true]
    )
    prec_gt, rec_gt, f1_gt, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    prec_fid, rec_fid, f1_fid, _ = precision_recall_fscore_support(
        gnn_pred, y_pred, average="weighted"
    )
    print(f"Test Accuracy (ground truth) (unweighted): {test_acc * 100:.2f}%")
    print(f"Test Accuracy (ground truth) (weighted): {weighted_accuracy * 100:.2f}%")
    print(f"Test Fidelity (vs GNN) (unweighted): {test_fid * 100:.2f}%")
    print(f"Test Fidelity (vs GNN) (weighted): {weighted_fidelity * 100:.2f}%")
    print(f"Test Precision (ground truth) (weighted): {prec_gt * 100:.2f}%")
    print(f"Test Recall (ground truth) (weighted): {rec_gt * 100:.2f}%")
    print(f"Test F1 Score (ground truth) (weighted): {f1_gt * 100:.2f}%")
    print(f"Test Precision (fidelity) (weighted): {prec_fid * 100:.2f}%")
    print(f"Test Recall (fidelity) (weighted): {rec_fid * 100:.2f}%")
    print(f"Test F1 Score (fidelity) (weighted): {f1_fid * 100:.2f}%")

    # -----------------------
    # Print results
    # -----------------------

    ## end(BAshape, IMDB, large3)
    # if not use_node_features:
    #     end_time = time.time()
    #     print(f"Time taken for {args.dataset}_{args.seed}_{args.arch} without node features: {end_time - start_time:.2f} seconds")
    #     exit()
    print("-------------------------Grounding and Evaluating-------------------------")
    used_alone_predicates, used_iso_predicate_node = analyze_used_predicate_nodes(
        predicate_node, predicate_to_idx, used_predicates
    )
    if args.plot:
        print("Plotting explanations for alone predicates...")
        for wl, v in used_alone_predicates.items():
            p, node_list = v
            graph_idx, center_index = node_list[0]

            # Create a new figure for each predicate
            plt.figure(figsize=(8, 6))
            plot_alone_predicate_explanation(
                p, graph_idx, center_index, train_x_dict, train_edge_dict, k=k_hops
            )
            plt.tight_layout()
            os.makedirs(f"{save_dir_root}/alone", exist_ok=True)
            plt.savefig(f"{save_dir_root}/alone/subgraph_explanation_p_{p}.png")
            plt.close()
        print(
            f"Finished plotting explanations for alone predicates. Saved at {save_dir_root}/alone"
        )
    iso_predicates_inference = {}

    used_iso_predicates = list(used_iso_predicate_node.keys())
    hashs = []

    if use_embed != 0:
        print("Used iso predicates:", used_iso_predicates)
        if args.plot:
            print("Plotting explanations for iso predicates...")
        for p in used_iso_predicates:
            h = explain_predicate_with_rules(
                p_idx=p,
                used_iso_predicate_node=used_iso_predicate_node,
                train_x_dict=train_x_dict,
                train_edge_dict=train_edge_dict,
                atom_type_dict=atom_type_dict,
                idx_predicates_mapping=idx_predicates_mapping,
                iso_predicates_inference=iso_predicates_inference,
                one_hot=one_hot,
                k_hops=k_hops,
                top_k_subgraph=5,
                save_dir=f"{save_dir_root}/iso",
                verbose=0,
                plot=args.plot,
            )
            hashs.append(h)
        if args.plot:
            print(
                f"Finished plotting explanations for iso predicates. Saved at {save_dir_root}/iso"
            )

    # print(hashs)
    # print("Final iso_predicates_inference:", iso_predicates_inference)

    data_grounded_pred_array = grounded_graph_predictions(
        test_dataset,
        test_x_dict,
        test_edge_dict,
        atom_type_dict,
        iso_predicates_inference,
        used_alone_predicates,
        predicates,
        clf_graph,
        k_hops=k_hops,
        one_hot=one_hot,
    )
    # Step 1: Convert y_tensor to a NumPy array
    y_true = gnn_test_pred_tensor.numpy().ravel()
    y_pred = data_grounded_pred_array
    fidelity = accuracy_score(y_true, y_pred)
    weighted_fidelity = accuracy_score(
        y_true, y_pred, sample_weight=[class_weights_map[label] for label in y_true]
    )
    prec, rec, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    accuracy = accuracy_score(test_y_tensor.numpy(), data_grounded_pred_array)
    weighted_accuracy = accuracy_score(
        test_y_tensor.numpy(),
        data_grounded_pred_array,
        sample_weight=[class_weights_map[label] for label in y_true],
    )
    ##########################
    ##########################
    correct_predictions_class_1 = (
        (data_grounded_pred_array == y_true) & (y_true == 1)
    ).sum()

    total_actual_class_1 = (y_true == 1).sum()
    print("Class 1 coverage: ")
    print(correct_predictions_class_1 / total_actual_class_1)

    correct_predictions_class_0 = (
        (data_grounded_pred_array == y_true) & (y_true == 0)
    ).sum()

    total_actual_class_0 = (y_true == 0).sum()
    print("Class 0 coverage: ")
    print(correct_predictions_class_0 / total_actual_class_0)
    #############
    print(f"Test Fidelity (unweighted): {fidelity * 100:.2f}%")
    print(f"Test Fidelity (Weighted Fidelity): {weighted_fidelity * 100:.2f}%")
    print(f"Test Accuracy (unweighted): {accuracy * 100:.2f}%")
    print(f"Test Accuracy (Weighted Accuracy): {weighted_accuracy * 100:.2f}%")
    print(f"Test Fidelity (Weighted Precision): {prec * 100:.2f}%")
    print(f"Test Fidelity (Weighted Recall): {rec * 100:.2f}%")
    print(f"Test Fidelity (Weighted F1 Score): {f1 * 100:.2f}%")
    end_time = time.time()
    print(
        f"Time taken for {args.dataset}_{args.seed}_{args.arch} without node features: {end_time - start_time:.2f} seconds"
    )


if __name__ == "__main__":
    main()
